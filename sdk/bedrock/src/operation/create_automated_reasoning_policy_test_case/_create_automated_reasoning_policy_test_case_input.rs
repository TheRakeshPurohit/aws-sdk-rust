// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
#[allow(missing_docs)] // documentation missing in model
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq)]
pub struct CreateAutomatedReasoningPolicyTestCaseInput {
    /// <p>The Amazon Resource Name (ARN) of the Automated Reasoning policy for which to create the test.</p>
    pub policy_arn: ::std::option::Option<::std::string::String>,
    /// <p>The output content that's validated by the Automated Reasoning policy. This represents the foundation model response that will be checked for accuracy.</p>
    pub guard_content: ::std::option::Option<::std::string::String>,
    /// <p>The input query or prompt that generated the content. This provides context for the validation.</p>
    pub query_content: ::std::option::Option<::std::string::String>,
    /// <p>The expected result of the Automated Reasoning check. Valid values include: , TOO_COMPLEX, and NO_TRANSLATIONS.</p>
    /// <ul>
    /// <li>
    /// <p><code>VALID</code> - The claims are true. The claims are implied by the premises and the Automated Reasoning policy. Given the Automated Reasoning policy and premises, it is not possible for these claims to be false. In other words, there are no alternative answers that are true that contradict the claims.</p></li>
    /// <li>
    /// <p><code>INVALID</code> - The claims are false. The claims are not implied by the premises and Automated Reasoning policy. Furthermore, there exists different claims that are consistent with the premises and Automated Reasoning policy.</p></li>
    /// <li>
    /// <p><code>SATISFIABLE</code> - The claims can be true or false. It depends on what assumptions are made for the claim to be implied from the premises and Automated Reasoning policy rules. In this situation, different assumptions can make input claims false and alternative claims true.</p></li>
    /// <li>
    /// <p><code>IMPOSSIBLE</code> - Automated Reasoning can’t make a statement about the claims. This can happen if the premises are logically incorrect, or if there is a conflict within the Automated Reasoning policy itself.</p></li>
    /// <li>
    /// <p><code>TRANSLATION_AMBIGUOUS</code> - Detected an ambiguity in the translation meant it would be unsound to continue with validity checking. Additional context or follow-up questions might be needed to get translation to succeed.</p></li>
    /// <li>
    /// <p><code>TOO_COMPLEX</code> - The input contains too much information for Automated Reasoning to process within its latency limits.</p></li>
    /// <li>
    /// <p><code>NO_TRANSLATIONS</code> - Identifies that some or all of the input prompt wasn't translated into logic. This can happen if the input isn't relevant to the Automated Reasoning policy, or if the policy doesn't have variables to model relevant input. If Automated Reasoning can't translate anything, you get a single <code>NO_TRANSLATIONS</code> finding. You might also see a <code>NO_TRANSLATIONS</code> (along with other findings) if some part of the validation isn't translated.</p></li>
    /// </ul>
    pub expected_aggregated_findings_result: ::std::option::Option<crate::types::AutomatedReasoningCheckResult>,
    /// <p>A unique, case-sensitive identifier to ensure that the operation completes no more than one time. If this token matches a previous request, Amazon Bedrock ignores the request, but does not return an error.</p>
    pub client_request_token: ::std::option::Option<::std::string::String>,
    /// <p>The minimum confidence level for logic validation. Content that meets the threshold is considered a high-confidence finding that can be validated.</p>
    pub confidence_threshold: ::std::option::Option<f64>,
}
impl CreateAutomatedReasoningPolicyTestCaseInput {
    /// <p>The Amazon Resource Name (ARN) of the Automated Reasoning policy for which to create the test.</p>
    pub fn policy_arn(&self) -> ::std::option::Option<&str> {
        self.policy_arn.as_deref()
    }
    /// <p>The output content that's validated by the Automated Reasoning policy. This represents the foundation model response that will be checked for accuracy.</p>
    pub fn guard_content(&self) -> ::std::option::Option<&str> {
        self.guard_content.as_deref()
    }
    /// <p>The input query or prompt that generated the content. This provides context for the validation.</p>
    pub fn query_content(&self) -> ::std::option::Option<&str> {
        self.query_content.as_deref()
    }
    /// <p>The expected result of the Automated Reasoning check. Valid values include: , TOO_COMPLEX, and NO_TRANSLATIONS.</p>
    /// <ul>
    /// <li>
    /// <p><code>VALID</code> - The claims are true. The claims are implied by the premises and the Automated Reasoning policy. Given the Automated Reasoning policy and premises, it is not possible for these claims to be false. In other words, there are no alternative answers that are true that contradict the claims.</p></li>
    /// <li>
    /// <p><code>INVALID</code> - The claims are false. The claims are not implied by the premises and Automated Reasoning policy. Furthermore, there exists different claims that are consistent with the premises and Automated Reasoning policy.</p></li>
    /// <li>
    /// <p><code>SATISFIABLE</code> - The claims can be true or false. It depends on what assumptions are made for the claim to be implied from the premises and Automated Reasoning policy rules. In this situation, different assumptions can make input claims false and alternative claims true.</p></li>
    /// <li>
    /// <p><code>IMPOSSIBLE</code> - Automated Reasoning can’t make a statement about the claims. This can happen if the premises are logically incorrect, or if there is a conflict within the Automated Reasoning policy itself.</p></li>
    /// <li>
    /// <p><code>TRANSLATION_AMBIGUOUS</code> - Detected an ambiguity in the translation meant it would be unsound to continue with validity checking. Additional context or follow-up questions might be needed to get translation to succeed.</p></li>
    /// <li>
    /// <p><code>TOO_COMPLEX</code> - The input contains too much information for Automated Reasoning to process within its latency limits.</p></li>
    /// <li>
    /// <p><code>NO_TRANSLATIONS</code> - Identifies that some or all of the input prompt wasn't translated into logic. This can happen if the input isn't relevant to the Automated Reasoning policy, or if the policy doesn't have variables to model relevant input. If Automated Reasoning can't translate anything, you get a single <code>NO_TRANSLATIONS</code> finding. You might also see a <code>NO_TRANSLATIONS</code> (along with other findings) if some part of the validation isn't translated.</p></li>
    /// </ul>
    pub fn expected_aggregated_findings_result(&self) -> ::std::option::Option<&crate::types::AutomatedReasoningCheckResult> {
        self.expected_aggregated_findings_result.as_ref()
    }
    /// <p>A unique, case-sensitive identifier to ensure that the operation completes no more than one time. If this token matches a previous request, Amazon Bedrock ignores the request, but does not return an error.</p>
    pub fn client_request_token(&self) -> ::std::option::Option<&str> {
        self.client_request_token.as_deref()
    }
    /// <p>The minimum confidence level for logic validation. Content that meets the threshold is considered a high-confidence finding that can be validated.</p>
    pub fn confidence_threshold(&self) -> ::std::option::Option<f64> {
        self.confidence_threshold
    }
}
impl ::std::fmt::Debug for CreateAutomatedReasoningPolicyTestCaseInput {
    fn fmt(&self, f: &mut ::std::fmt::Formatter<'_>) -> ::std::fmt::Result {
        let mut formatter = f.debug_struct("CreateAutomatedReasoningPolicyTestCaseInput");
        formatter.field("policy_arn", &self.policy_arn);
        formatter.field("guard_content", &"*** Sensitive Data Redacted ***");
        formatter.field("query_content", &"*** Sensitive Data Redacted ***");
        formatter.field("expected_aggregated_findings_result", &self.expected_aggregated_findings_result);
        formatter.field("client_request_token", &self.client_request_token);
        formatter.field("confidence_threshold", &self.confidence_threshold);
        formatter.finish()
    }
}
impl CreateAutomatedReasoningPolicyTestCaseInput {
    /// Creates a new builder-style object to manufacture [`CreateAutomatedReasoningPolicyTestCaseInput`](crate::operation::create_automated_reasoning_policy_test_case::CreateAutomatedReasoningPolicyTestCaseInput).
    pub fn builder() -> crate::operation::create_automated_reasoning_policy_test_case::builders::CreateAutomatedReasoningPolicyTestCaseInputBuilder {
        crate::operation::create_automated_reasoning_policy_test_case::builders::CreateAutomatedReasoningPolicyTestCaseInputBuilder::default()
    }
}

/// A builder for [`CreateAutomatedReasoningPolicyTestCaseInput`](crate::operation::create_automated_reasoning_policy_test_case::CreateAutomatedReasoningPolicyTestCaseInput).
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default)]
#[non_exhaustive]
pub struct CreateAutomatedReasoningPolicyTestCaseInputBuilder {
    pub(crate) policy_arn: ::std::option::Option<::std::string::String>,
    pub(crate) guard_content: ::std::option::Option<::std::string::String>,
    pub(crate) query_content: ::std::option::Option<::std::string::String>,
    pub(crate) expected_aggregated_findings_result: ::std::option::Option<crate::types::AutomatedReasoningCheckResult>,
    pub(crate) client_request_token: ::std::option::Option<::std::string::String>,
    pub(crate) confidence_threshold: ::std::option::Option<f64>,
}
impl CreateAutomatedReasoningPolicyTestCaseInputBuilder {
    /// <p>The Amazon Resource Name (ARN) of the Automated Reasoning policy for which to create the test.</p>
    /// This field is required.
    pub fn policy_arn(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.policy_arn = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The Amazon Resource Name (ARN) of the Automated Reasoning policy for which to create the test.</p>
    pub fn set_policy_arn(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.policy_arn = input;
        self
    }
    /// <p>The Amazon Resource Name (ARN) of the Automated Reasoning policy for which to create the test.</p>
    pub fn get_policy_arn(&self) -> &::std::option::Option<::std::string::String> {
        &self.policy_arn
    }
    /// <p>The output content that's validated by the Automated Reasoning policy. This represents the foundation model response that will be checked for accuracy.</p>
    /// This field is required.
    pub fn guard_content(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.guard_content = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The output content that's validated by the Automated Reasoning policy. This represents the foundation model response that will be checked for accuracy.</p>
    pub fn set_guard_content(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.guard_content = input;
        self
    }
    /// <p>The output content that's validated by the Automated Reasoning policy. This represents the foundation model response that will be checked for accuracy.</p>
    pub fn get_guard_content(&self) -> &::std::option::Option<::std::string::String> {
        &self.guard_content
    }
    /// <p>The input query or prompt that generated the content. This provides context for the validation.</p>
    pub fn query_content(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.query_content = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The input query or prompt that generated the content. This provides context for the validation.</p>
    pub fn set_query_content(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.query_content = input;
        self
    }
    /// <p>The input query or prompt that generated the content. This provides context for the validation.</p>
    pub fn get_query_content(&self) -> &::std::option::Option<::std::string::String> {
        &self.query_content
    }
    /// <p>The expected result of the Automated Reasoning check. Valid values include: , TOO_COMPLEX, and NO_TRANSLATIONS.</p>
    /// <ul>
    /// <li>
    /// <p><code>VALID</code> - The claims are true. The claims are implied by the premises and the Automated Reasoning policy. Given the Automated Reasoning policy and premises, it is not possible for these claims to be false. In other words, there are no alternative answers that are true that contradict the claims.</p></li>
    /// <li>
    /// <p><code>INVALID</code> - The claims are false. The claims are not implied by the premises and Automated Reasoning policy. Furthermore, there exists different claims that are consistent with the premises and Automated Reasoning policy.</p></li>
    /// <li>
    /// <p><code>SATISFIABLE</code> - The claims can be true or false. It depends on what assumptions are made for the claim to be implied from the premises and Automated Reasoning policy rules. In this situation, different assumptions can make input claims false and alternative claims true.</p></li>
    /// <li>
    /// <p><code>IMPOSSIBLE</code> - Automated Reasoning can’t make a statement about the claims. This can happen if the premises are logically incorrect, or if there is a conflict within the Automated Reasoning policy itself.</p></li>
    /// <li>
    /// <p><code>TRANSLATION_AMBIGUOUS</code> - Detected an ambiguity in the translation meant it would be unsound to continue with validity checking. Additional context or follow-up questions might be needed to get translation to succeed.</p></li>
    /// <li>
    /// <p><code>TOO_COMPLEX</code> - The input contains too much information for Automated Reasoning to process within its latency limits.</p></li>
    /// <li>
    /// <p><code>NO_TRANSLATIONS</code> - Identifies that some or all of the input prompt wasn't translated into logic. This can happen if the input isn't relevant to the Automated Reasoning policy, or if the policy doesn't have variables to model relevant input. If Automated Reasoning can't translate anything, you get a single <code>NO_TRANSLATIONS</code> finding. You might also see a <code>NO_TRANSLATIONS</code> (along with other findings) if some part of the validation isn't translated.</p></li>
    /// </ul>
    /// This field is required.
    pub fn expected_aggregated_findings_result(mut self, input: crate::types::AutomatedReasoningCheckResult) -> Self {
        self.expected_aggregated_findings_result = ::std::option::Option::Some(input);
        self
    }
    /// <p>The expected result of the Automated Reasoning check. Valid values include: , TOO_COMPLEX, and NO_TRANSLATIONS.</p>
    /// <ul>
    /// <li>
    /// <p><code>VALID</code> - The claims are true. The claims are implied by the premises and the Automated Reasoning policy. Given the Automated Reasoning policy and premises, it is not possible for these claims to be false. In other words, there are no alternative answers that are true that contradict the claims.</p></li>
    /// <li>
    /// <p><code>INVALID</code> - The claims are false. The claims are not implied by the premises and Automated Reasoning policy. Furthermore, there exists different claims that are consistent with the premises and Automated Reasoning policy.</p></li>
    /// <li>
    /// <p><code>SATISFIABLE</code> - The claims can be true or false. It depends on what assumptions are made for the claim to be implied from the premises and Automated Reasoning policy rules. In this situation, different assumptions can make input claims false and alternative claims true.</p></li>
    /// <li>
    /// <p><code>IMPOSSIBLE</code> - Automated Reasoning can’t make a statement about the claims. This can happen if the premises are logically incorrect, or if there is a conflict within the Automated Reasoning policy itself.</p></li>
    /// <li>
    /// <p><code>TRANSLATION_AMBIGUOUS</code> - Detected an ambiguity in the translation meant it would be unsound to continue with validity checking. Additional context or follow-up questions might be needed to get translation to succeed.</p></li>
    /// <li>
    /// <p><code>TOO_COMPLEX</code> - The input contains too much information for Automated Reasoning to process within its latency limits.</p></li>
    /// <li>
    /// <p><code>NO_TRANSLATIONS</code> - Identifies that some or all of the input prompt wasn't translated into logic. This can happen if the input isn't relevant to the Automated Reasoning policy, or if the policy doesn't have variables to model relevant input. If Automated Reasoning can't translate anything, you get a single <code>NO_TRANSLATIONS</code> finding. You might also see a <code>NO_TRANSLATIONS</code> (along with other findings) if some part of the validation isn't translated.</p></li>
    /// </ul>
    pub fn set_expected_aggregated_findings_result(mut self, input: ::std::option::Option<crate::types::AutomatedReasoningCheckResult>) -> Self {
        self.expected_aggregated_findings_result = input;
        self
    }
    /// <p>The expected result of the Automated Reasoning check. Valid values include: , TOO_COMPLEX, and NO_TRANSLATIONS.</p>
    /// <ul>
    /// <li>
    /// <p><code>VALID</code> - The claims are true. The claims are implied by the premises and the Automated Reasoning policy. Given the Automated Reasoning policy and premises, it is not possible for these claims to be false. In other words, there are no alternative answers that are true that contradict the claims.</p></li>
    /// <li>
    /// <p><code>INVALID</code> - The claims are false. The claims are not implied by the premises and Automated Reasoning policy. Furthermore, there exists different claims that are consistent with the premises and Automated Reasoning policy.</p></li>
    /// <li>
    /// <p><code>SATISFIABLE</code> - The claims can be true or false. It depends on what assumptions are made for the claim to be implied from the premises and Automated Reasoning policy rules. In this situation, different assumptions can make input claims false and alternative claims true.</p></li>
    /// <li>
    /// <p><code>IMPOSSIBLE</code> - Automated Reasoning can’t make a statement about the claims. This can happen if the premises are logically incorrect, or if there is a conflict within the Automated Reasoning policy itself.</p></li>
    /// <li>
    /// <p><code>TRANSLATION_AMBIGUOUS</code> - Detected an ambiguity in the translation meant it would be unsound to continue with validity checking. Additional context or follow-up questions might be needed to get translation to succeed.</p></li>
    /// <li>
    /// <p><code>TOO_COMPLEX</code> - The input contains too much information for Automated Reasoning to process within its latency limits.</p></li>
    /// <li>
    /// <p><code>NO_TRANSLATIONS</code> - Identifies that some or all of the input prompt wasn't translated into logic. This can happen if the input isn't relevant to the Automated Reasoning policy, or if the policy doesn't have variables to model relevant input. If Automated Reasoning can't translate anything, you get a single <code>NO_TRANSLATIONS</code> finding. You might also see a <code>NO_TRANSLATIONS</code> (along with other findings) if some part of the validation isn't translated.</p></li>
    /// </ul>
    pub fn get_expected_aggregated_findings_result(&self) -> &::std::option::Option<crate::types::AutomatedReasoningCheckResult> {
        &self.expected_aggregated_findings_result
    }
    /// <p>A unique, case-sensitive identifier to ensure that the operation completes no more than one time. If this token matches a previous request, Amazon Bedrock ignores the request, but does not return an error.</p>
    pub fn client_request_token(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.client_request_token = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>A unique, case-sensitive identifier to ensure that the operation completes no more than one time. If this token matches a previous request, Amazon Bedrock ignores the request, but does not return an error.</p>
    pub fn set_client_request_token(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.client_request_token = input;
        self
    }
    /// <p>A unique, case-sensitive identifier to ensure that the operation completes no more than one time. If this token matches a previous request, Amazon Bedrock ignores the request, but does not return an error.</p>
    pub fn get_client_request_token(&self) -> &::std::option::Option<::std::string::String> {
        &self.client_request_token
    }
    /// <p>The minimum confidence level for logic validation. Content that meets the threshold is considered a high-confidence finding that can be validated.</p>
    pub fn confidence_threshold(mut self, input: f64) -> Self {
        self.confidence_threshold = ::std::option::Option::Some(input);
        self
    }
    /// <p>The minimum confidence level for logic validation. Content that meets the threshold is considered a high-confidence finding that can be validated.</p>
    pub fn set_confidence_threshold(mut self, input: ::std::option::Option<f64>) -> Self {
        self.confidence_threshold = input;
        self
    }
    /// <p>The minimum confidence level for logic validation. Content that meets the threshold is considered a high-confidence finding that can be validated.</p>
    pub fn get_confidence_threshold(&self) -> &::std::option::Option<f64> {
        &self.confidence_threshold
    }
    /// Consumes the builder and constructs a [`CreateAutomatedReasoningPolicyTestCaseInput`](crate::operation::create_automated_reasoning_policy_test_case::CreateAutomatedReasoningPolicyTestCaseInput).
    pub fn build(
        self,
    ) -> ::std::result::Result<
        crate::operation::create_automated_reasoning_policy_test_case::CreateAutomatedReasoningPolicyTestCaseInput,
        ::aws_smithy_types::error::operation::BuildError,
    > {
        ::std::result::Result::Ok(
            crate::operation::create_automated_reasoning_policy_test_case::CreateAutomatedReasoningPolicyTestCaseInput {
                policy_arn: self.policy_arn,
                guard_content: self.guard_content,
                query_content: self.query_content,
                expected_aggregated_findings_result: self.expected_aggregated_findings_result,
                client_request_token: self.client_request_token,
                confidence_threshold: self.confidence_threshold,
            },
        )
    }
}
impl ::std::fmt::Debug for CreateAutomatedReasoningPolicyTestCaseInputBuilder {
    fn fmt(&self, f: &mut ::std::fmt::Formatter<'_>) -> ::std::fmt::Result {
        let mut formatter = f.debug_struct("CreateAutomatedReasoningPolicyTestCaseInputBuilder");
        formatter.field("policy_arn", &self.policy_arn);
        formatter.field("guard_content", &"*** Sensitive Data Redacted ***");
        formatter.field("query_content", &"*** Sensitive Data Redacted ***");
        formatter.field("expected_aggregated_findings_result", &self.expected_aggregated_findings_result);
        formatter.field("client_request_token", &self.client_request_token);
        formatter.field("confidence_threshold", &self.confidence_threshold);
        formatter.finish()
    }
}
