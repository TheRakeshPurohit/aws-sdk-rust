// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
impl super::Client {
    /// Constructs a fluent builder for the [`GetTrainedModelInferenceJob`](crate::operation::get_trained_model_inference_job::builders::GetTrainedModelInferenceJobFluentBuilder) operation.
    ///
    /// - The fluent builder is configurable:
    ///   - [`membership_identifier(impl Into<String>)`](crate::operation::get_trained_model_inference_job::builders::GetTrainedModelInferenceJobFluentBuilder::membership_identifier) / [`set_membership_identifier(Option<String>)`](crate::operation::get_trained_model_inference_job::builders::GetTrainedModelInferenceJobFluentBuilder::set_membership_identifier):<br>required: **true**<br><p>Provides the membership ID of the membership that contains the trained model inference job that you are interested in.</p><br>
    ///   - [`trained_model_inference_job_arn(impl Into<String>)`](crate::operation::get_trained_model_inference_job::builders::GetTrainedModelInferenceJobFluentBuilder::trained_model_inference_job_arn) / [`set_trained_model_inference_job_arn(Option<String>)`](crate::operation::get_trained_model_inference_job::builders::GetTrainedModelInferenceJobFluentBuilder::set_trained_model_inference_job_arn):<br>required: **true**<br><p>Provides the Amazon Resource Name (ARN) of the trained model inference job that you are interested in.</p><br>
    /// - On success, responds with [`GetTrainedModelInferenceJobOutput`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput) with field(s):
    ///   - [`create_time(DateTime)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::create_time): <p>The time at which the trained model inference job was created.</p>
    ///   - [`update_time(DateTime)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::update_time): <p>The most recent time at which the trained model inference job was updated.</p>
    ///   - [`trained_model_inference_job_arn(String)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::trained_model_inference_job_arn): <p>The Amazon Resource Name (ARN) of the trained model inference job.</p>
    ///   - [`configured_model_algorithm_association_arn(Option<String>)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::configured_model_algorithm_association_arn): <p>The Amazon Resource Name (ARN) of the configured model algorithm association that was used for the trained model inference job.</p>
    ///   - [`name(String)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::name): <p>The name of the trained model inference job.</p>
    ///   - [`status(TrainedModelInferenceJobStatus)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::status): <p>The status of the trained model inference job.</p>
    ///   - [`trained_model_arn(String)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::trained_model_arn): <p>The Amazon Resource Name (ARN) for the trained model that was used for the trained model inference job.</p>
    ///   - [`trained_model_version_identifier(Option<String>)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::trained_model_version_identifier): <p>The version identifier of the trained model used for this inference job. This identifies the specific version of the trained model that was used to generate the inference results.</p>
    ///   - [`resource_config(Option<InferenceResourceConfig>)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::resource_config): <p>The resource configuration information for the trained model inference job.</p>
    ///   - [`output_configuration(Option<InferenceOutputConfiguration>)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::output_configuration): <p>The output configuration information for the trained model inference job.</p>
    ///   - [`membership_identifier(String)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::membership_identifier): <p>The membership ID of the membership that contains the trained model inference job.</p>
    ///   - [`data_source(Option<ModelInferenceDataSource>)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::data_source): <p>The data source that was used for the trained model inference job.</p>
    ///   - [`container_execution_parameters(Option<InferenceContainerExecutionParameters>)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::container_execution_parameters): <p>The execution parameters for the model inference job container.</p>
    ///   - [`status_details(Option<StatusDetails>)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::status_details): <p>Details about the status of a resource.</p>
    ///   - [`description(Option<String>)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::description): <p>The description of the trained model inference job.</p>
    ///   - [`inference_container_image_digest(Option<String>)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::inference_container_image_digest): <p>Information about the training container image.</p>
    ///   - [`environment(Option<HashMap::<String, String>>)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::environment): <p>The environment variables to set in the Docker container.</p>
    ///   - [`kms_key_arn(Option<String>)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::kms_key_arn): <p>The Amazon Resource Name (ARN) of the KMS key. This key is used to encrypt and decrypt customer-owned data in the ML inference job and associated data.</p>
    ///   - [`metrics_status(Option<MetricsStatus>)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::metrics_status): <p>The metrics status for the trained model inference job.</p>
    ///   - [`metrics_status_details(Option<String>)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::metrics_status_details): <p>Details about the metrics status for the trained model inference job.</p>
    ///   - [`logs_status(Option<LogsStatus>)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::logs_status): <p>The logs status for the trained model inference job.</p>
    ///   - [`logs_status_details(Option<String>)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::logs_status_details): <p>Details about the logs status for the trained model inference job.</p>
    ///   - [`tags(Option<HashMap::<String, String>>)`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobOutput::tags): <p>The optional metadata that you applied to the resource to help you categorize and organize them. Each tag consists of a key and an optional value, both of which you define.</p> <p>The following basic restrictions apply to tags:</p> <ul>  <li>   <p>Maximum number of tags per resource - 50.</p></li>  <li>   <p>For each resource, each tag key must be unique, and each tag key can have only one value.</p></li>  <li>   <p>Maximum key length - 128 Unicode characters in UTF-8.</p></li>  <li>   <p>Maximum value length - 256 Unicode characters in UTF-8.</p></li>  <li>   <p>If your tagging schema is used across multiple services and resources, remember that other services may have restrictions on allowed characters. Generally allowed characters are: letters, numbers, and spaces representable in UTF-8, and the following characters: + - = . _ : / @.</p></li>  <li>   <p>Tag keys and values are case sensitive.</p></li>  <li>   <p>Do not use aws:, AWS:, or any upper or lowercase combination of such as a prefix for keys as it is reserved for AWS use. You cannot edit or delete tag keys with this prefix. Values can have this prefix. If a tag value has aws as its prefix but the key does not, then Clean Rooms ML considers it to be a user tag and will count against the limit of 50 tags. Tags with only the key prefix of aws do not count against your tags per resource limit.</p></li> </ul>
    /// - On failure, responds with [`SdkError<GetTrainedModelInferenceJobError>`](crate::operation::get_trained_model_inference_job::GetTrainedModelInferenceJobError)
    pub fn get_trained_model_inference_job(
        &self,
    ) -> crate::operation::get_trained_model_inference_job::builders::GetTrainedModelInferenceJobFluentBuilder {
        crate::operation::get_trained_model_inference_job::builders::GetTrainedModelInferenceJobFluentBuilder::new(self.handle.clone())
    }
}
